/** \page extending Writing New Skeleton Apps and Extending SST/macro

Unlike trace simulations, which are limited by the scale of traces which can be collected on existing hardware, skeleton apps either reproduce or approximate the control flow of a real application and thus have no inherent limits to the scale which can be simulated.
A collection of tutorial programs demonstrating the mechanisms for writing skeleton apps and extending SST/macro has been placed in the top level <tt>tutorials</tt> directory of the distribution.
These examples are a good starting point for developing with SST/macro.
The tutorials are constructed such that they can be moved to anywhere within your filesystem, modified if desired and built using the supplied makefiles, provided that SST/macro is installed and that the <tt>sst++</tt> compiler wrapper is found within your search path.

\section mpi_skeletons MPI Skeletons

While SST/macro is by no means limited to only simulating the MPI programming model, reviewing the various options for creating MPI-based skeletons is highly instructive.
A number of example MPI skeletons are examined in the following discussion.

<ul>
    <li> \ref integrator
    <li> \ref sendrecv_c
    <li> \ref sendrecv_cxx
    <li> \ref sendrecv_cxx2
</ul>

\subsection integrator 1D Integrator (C MPI Interface)

The program found in <tt>tutorials/1d_integrator_c</tt> is a C program that performs a simple integration of a given function -- an extremely simple example which nevertheless performs a real parallel computation.
As distributed it calculates the integral of <tt>x^2</tt> from <tt>x=0</tt> to <tt>x=1</tt> (the analytical solution is 1/3), though this can easily changed by modifying the source code.
Since efforts are being made to minimize the differences between real MPI applications and SST/macro skeleton apps, one approach to developing skeleton apps is to maintain a single source file and use preprocessing to switch the code between functioning as a real MPI app and running within the simulator.
For our purposes here, it is quite instructive to examine the original MPI code and modified code side-by-side as the preprocessing directives allow.

The first required modification is replacement of the <tt>mpi.h</tt> header with the SST/macro MPI header as follows.
We also include the <tt>sstmac_compute.h</tt> header to allow access to some compute modeling capabilities.

\dontinclude 1d_integrator_c/1d_integrator.c
\skip #ifndef
\until #endif

Next, we need to rename <tt>main</tt> such that the simulator can call into the skeleton app at the appropriate time.
Note that <tt>user_skeleton_main</tt> is the required name; rename <tt>main</tt> to this and use the supplied makefile and everything will work automatically.

\skip #ifndef
\until #endif

The SST/macro C MPI interface is identical to the real MPI interface, so the original MPI code remains unmodified.

\skip MPI_Init
\until MPI_Comm_size

Each instance of a skeleton application is run within the simulator as a lightweight user space thread and could execute any arbitrary code including heavy computation loads.
Performing significant amounts of computation would, however, make simulations run extremely slowly as the threads execute in serial.
Thus, computation must be abstracted as much as possible.
In this example, the single computation block is replaced by a call to <tt>SSTMAC_compute</tt> which blocks the thread until the given amount of simulation time elapses.
This simulation time is determined by computing the number of times the real computation loop would iterate for the given parameters and naively guessing that each loop iteration requires 0.1 us.

\skip #ifndef
\until #endif

Two caveats about compute modeling should be pointed out.
First, computation modeling is challenging and this illustrative example obviously leaves much to be desired in terms of realistic compuation simulation.
Second, SST/macro currently only has a very limited prototype compute interface available for the C language.

Lastly, code for the <tt>MPI_Reduce</tt> is unchanged and is followed by printing the result and calling <tt>MPI_Finalize</tt>.

\skip MPI_Reduce
\until MPI_Finalize

At this point, readers should try to build (<tt>make</tt>) and run the executable (<tt>./runsstmac</tt>) produced.
Note that the executable produced is essentially the standard <tt>sstmac</tt> executable with the user supplied skeleton app linked in.
Examination of the help (<tt>./runsstmac --help</tt>) demonstrates this; see the help for details on running the simulation.

SST/macro can actually send data via its MPI implementation if required.
This could be useful if a skeleton app's control flow depends on data shared between nodes.
By default, the SST/macro MPI implementation will send data if a non-null pointer is encountered (as in this example).
However, the <tt>parameter.ini</tt> file for this example contains a parameter turning off payloads globally (so the final value for <tt>total</tt> is the initial value of -1.0).

\dontinclude 1d_integrator_c/parameters.ini
\skip # MPI library
\until true

Changing this parameter to false, the <tt>area</tt> variables initialized to 0 will be summed into <tt>total</tt>, resulting in a final <tt>total</tt> value of 0.
But if the preprocessor directives around the computation blocks are removed, allowing both the computation and the computation modeling to take place, the correct answer will be computed by the simulation.
Note that the elapsed simulation time is uneffected by any of these changes.

Also take note in the parameter file that <tt>user_mpiapp_c</tt> is the app which the simulator actually runs.  
This is simply a C++ wrapper app that calls into the <tt>user_skeleton_main</tt> supplied by the user.

\dontinclude 1d_integrator_c/parameters.ini
\skip launch_app1
\until user_mpiapp_c

One final suggested experiment is to modify the makefile to build this example as a real MPI application  by filling in <tt>CC</tt> with an MPI-capable C compiler and commenting/uncommenting lines as directed.
Build and run it for yourself.

\subsection sendrecv_c Send/Recv Example (C MPI Interface)

The C language send/recv (<tt>tutorials/sendrecv_c</tt>) example is the first in a series of skeleton apps illustrating the different approaches that can be used for developing skeleton apps in various languages.
Coded only for the simulator (no preprocessing) and performing only a simple send/recv pair, these examples are even simpler than the 1D integrator code.

The code for the MPI send/recv pair follows.  Note that NULL buffer pointers are passed as argmuments to the MPI functions, ensuring that no data will be transferred by the simulator's MPI implementation.

\dontinclude sendrecv_c/sendrecv.c
\skip if (me == 0) {
\until }
\skip else
\until }

\subsection sendrecv_cxx Send/Recv Example (C++ using C MPI Interface)

The next example, found in <tt>tutorials/sendrecv_cxx</tt> modifies the C language send/recv example to use C++ and take advantage of capabilities which the simulator can provide to C++ skeletons. 

For simplicity the <tt>sstmacro.h</tt> header is included which pulls in the collection of all useful SST/macro headers.

\dontinclude sendrecv_cxx/sendrecv.cc
\skip #include
\until <sstmac/sstmacro.h>

While the MPI code is completely unchanged, a <tt>consume_parameters</tt> function is added which accepts and stores a pointer to a parameters object.

\skip sim_parameters
\until }

SST/macro calls the <tt>consume_parameters</tt> function prior to calling <tt>user_skeleton_main</tt>, so that the parameters object can be used to configure the application (see <tt>src/sprockit/sim_parameters.h</tt>).
In this example, the <tt>sendrecv_message_size</tt> parameter is used to configure the size of message used in the send/recv simulation.

\skip int message_size
\until "sendrecv_message_size");

Note in the <tt>parameters.ini</tt> file that we are now running the <tt>user_mpiapp_cxx</tt> app.

\dontinclude sendrecv_cxx/parameters.ini
\skip launch_app1
\until user_mpiapp_cxx

\subsection sendrecv_cxx2 Send/Recv Example (Full C++)

While the SST/macro C MPI interface is complete, numerous more advanced features (in particular compute modeling) are only available to C++ applications (as demonstrated with parameter processing in the previous example).
The full C++ example, found in <tt>tutorials/sendrecv_cxx2</tt>, demonstrates how to implement a skeleton through inheritance from the <tt>sstmac::sw::mpiapp</tt> class provided by SST/macro.
Following the example here, it should be pretty easy to develop a skeleton app in this manner, though the benefits to doing so aren't necessarily obvious.
As a completely separate concern, SST/macro's C++ MPI interface, which is functionally identical but syntactically different from the MPI standard C++ interface, is also used in this example.
Prior to recent efforts in streamlining skeleton generation and the addition of MPI standard language bindings, all skeleton apps were written as shown here, so grasping this example will aid in understanding many of the existing SST/macro skeletons.
SST/macro's non-standard C++ MPI interface probably has programmability advantages, though the disadvantage of having to learn a new and slightly different interface certainly counters those advantages; the choice is left to the user.

Internally, SST/macro C and Fortran interfaces are just wrappers for C++ implementations, so any of the language interfaces can be mixed and matched within an application.
That is to say, this full C++ example could just as well have used the C MPI interface and not changed the MPI code from the previous send/recv examples and it is completely reasonable to use the C MPI interfaces from a C++ skeleton implementation and use the more capable C++ interfaces for tasks such as parameter processing or compute modeling.

Again, start by including the <tt>sstmacro.h</tt> header.
The send/recv application exists in the application-specific namespace <tt>sendrecv</tt>. 
All of the useful SST/macro types will exist in the <tt>sstmac</tt> or <tt>sstmac::sw</tt> namespaces.
Many of these, such as the <tt>sstmac::sw::mpiapp</tt> class which MPI skeletons inherit from, will be automatically brought into the global namespace.

\dontinclude sendrecv_cxx2/sendrecv.cc
\skip include
\until sendrecv_skeleton

First, a few typedefs are defined.
The header library <tt>sprockit::refcount_ptr</tt> is used by SST/macro to handle memory management.
Memory-managed classes typedef the intrusive pointer type as <tt>ptr</tt> and similarly the <tt>const_ptr</tt>.

\skip typedef
\until const_ptr

Having finished the memory management the constructor is needed.

\skip Constructor
\until }

All skeleton constructors expect a software id to label the application.
These parameters along with the class name are passed along to the parent <tt>mpiapp</tt> class. So far so good.

The next required methods are the <tt>construct</tt> and <tt>clone</tt> methods.
These will be used by SST/macro for generating copies of your application on all the nodes in the simulation.

\skip sendrecv_skeleton::ptr
\until clone
\until }

When the application is constructed, a method is needed to read any
user-specified parameters. In this case, the method just reads an integer
parameter named <tt>message_size</tt>.

\skip consume
\until }

Also add a simple <tt>to_string</tt> method for debug printing.
\skip to_string
\until }

Finally the actual application is included.
Because the class inherits from <tt>mpiapp</tt>, the MPI interface is immediately available without any extra work.

\skip skeleton_main
\until finalize
\until }

The only thing left to do is register the application with the SST/macro runtime.
This is mediated by a static class descriptor which is simplifed through a macro.

\skipline SpktRegister

The macro should be registered in the appropriate namespace.  The macro requires 2 arguments.  
First, a string identifier must be given.
Second, the name of the class is given. 
In this case, we have used the namespace <tt>sendrecv</tt>. IMPORTANT!
Do not use colons in the class name. No namespaces should be prefixed. The class
name is used to automatically generate a series of C++ objects.  This will upset the compiler.
Note that the class name can now be used in the input file to launch the application rather than using the <tt>user_mpiapp_c</tt>/<tt>user_mpiapp_cxx</tt> wrapper apps.

\dontinclude sendrecv_cxx2/parameters.ini
\skip launch_app1
\until sendrecv

As before, <tt>make</tt> generates the executable to run the simulation.

*/

