/** \page gettingstarted Getting Started with SST/macro

This page provides a brief overview of how to use SST/macro.


<ul>
    <li> \ref inputfile
    <li> \ref machinemodel
    <li> \ref complicated_input
</ul>

An overview of basic input files and examples of
running the simulator are given. The basic tools for collecting machine
parameters is also demonstrated.  Please refer to the \ref compile section for
instructions on compiling and installing SST/macro. Further tutorials
will focus on using the DUMPI tracer for existing applications (\ref dumpitracer),
and writing basic skeleton applications (\ref extending).

\section inputfile The Basic SST/macro Input File

Show below is the set of basic parameters needed to run SST/macro. In this case, we 
launch a simple MPI ping-pong program. For simplicity, the tutorial is based on the Cray
XT4. The complete set of parameters for running the job are given below:
<ul>
<li> # Network parameters
<li> network_name = shared
<li> network_latency = 5830ns
<li> network_bandwidth_link = 3219mbytes/sec
<li> network_bandwidth_node = 1722mbytes/sec
<li> topology_geometry = 17,24,24
<li> topology_name = hdtorus
<li> nic_name = simple
<li>
<li> # Node parameters
<li> node_name = simple
<li> node_cores = 4
<li> node_mem_bandwidth = 4886mbytes/sec
<li> node_mem_latency = 390ns
<li> node_frequency = 2.3e9
<li>
<li> # Launch parameters
<li> launch_name = instant
<li> launch_indexing = block
<li> launch_allocation = first_available
<li> launch_app1 = MPI_pingpong
<li> launch_app1_size = 2
<li> launch_app1_start = 0ms
<li> launch_app1_tasks_per_node = 1
<li> 
<li> # Application parameters
<li> mpipingpong_iterations = 100
<li> mpipingpong_count = 1000
</ul>
A complete listing and description of relevant keywords can be found in \ref keywords .

In the above input file, we first specify the type of network 
(<tt>network_name = shared</tt>). We use a shared-circuit model allowing shared or 
simultaneous messages across network edges. The shared-circuit model 
further requires a node-to-node latency, a node-to-router bandwidth 
(<tt>network_bandwidth_node</tt>), and a router-to-router link bandwidth (<tt>network_bandwidth_link</tt>). 
Once the interconnect speeds are specified, the network topology must be 
specified.  For Franklin, we specifiy an hdtorus running along three Cartesian directions.
The torus has size 17 along the x-axis, 24 along the y-axis, and 24 along the z-axis.
Finally, we use the simple network interface controller (<tt>nic_name = simple</tt>).  
To construct the nodes, we specify a simple node 
model (<tt>node_name = simple</tt>). On Franklin, there are 4 cores per node. Finally, we give the node
specs for 
<tt>node_mem_latency</tt>,
<tt>node_mem_bandwidth</tt>, and
<tt>node_frequency</tt>

Once we have built a machine model, we must specify the application to launch.  
In most parallel applications launched with e.g. aprun or Slurm, the launcher immediately 
starts a single application on each node. Thus we specify the launch behavior as 
<tt>launch_name = instant</tt>. In our case, there are 9692 total nodes, each of which 
must be assigned an index from 0 to 9691.  Once node indices are assigned, 
particular nodes should be allocated to run the application.  Basic indexing
and allocation behaviors are specified with
<tt>launch_indexing = block</tt> and <tt>launch_allocation = first_available</tt>.
The application
<tt>launch_app1 = MPI_pingpong</tt>
and number of instances
<tt>launch_app1_size = 2</tt>
are then specified. An optional launch delay can also be included. These parameters can also
be replaced by using a simulated <tt>aprun</tt> launch common on many supercomputing platforms.
In this case, the keywords <tt>launch_app1</tt>, <tt>launch_app1_size</tt>, 
<tt>launch_app1_ntask_per_node</tt>, and <tt>launch_app1_start</tt> are all replaced by
the single line
<pre>
launch_app1 = aprun -n2 -N1 MPI_pingpong
</pre>
with the aprun syntax exactly as that for the standard Cray aprun utility.  The
aprun input also then simulates a delay on certain nodes based on when aprun
is expected to actually launch the process.


We are now almost ready to run the simulation. The application itself requires 
a few parameters. In this ping-pong example, we will send 100 messages of size
1000 bytes back and forth between two nodes.  Assuming the paramaters are in a file
named <tt>parameters.ini</tt>, we simply go to the folder and run

<pre>
sstmac -f parameters.ini
</pre>

\section machinemodel Constructing a Machine Model

We now walk through the steps used to construct the above machine model for Franklin. 
First, a file describing the layout of nodes within Franklin's torus is required.
SST/Macro provides the <tt>bin/xt2nodemap.pl</tt> script for processing the output of Cray's <tt>xtdb2proc</tt>.  
The resulting nodemap file will be one of the inputs to the simulator.

<pre>
xtdb2proc -f - > franklin-db.txt;
xt2nodemap.pl -t hdtorus < franklin-db.txt > franklin-nodemap.txt
</pre>

The xt2nodemap.pl script is unable to determine the topology from the cray db file.
Depending on whether a 3D torus (torus3) or higher-dimension torus (hdtorus) topology
is used, the indexing of the nodes will change, e.g. torus3 requires six indices
(three x,y,z node coordinates and three x,y,z switch coordinates) while the hdtorus
require four indices (three x,y,z switch coordinates and one node coordinate). The
topology specified by the -t option should match the topology given below in the
parameters.ini file for the SST/Macro simulation (vide infra). At the top of
the franklin-nodemap.txt file, there will be a comment line identifying the dimensions
of the network that will be used later in the input file.  In this case, the mesh size
of the network is given as 17,24,24. <br>
IMPORTANT: The topology name specified in the .ini file must match the topology name specfied
when the xt2nodemap.pl script was run (vide supra). If hdtorus was used in xt2nodemap.pl, then
the topology in the input file must be hdtorus or hdtorus3d.

A set of MPI microbenchmarks is distributed with the SST/Macro source, located
in <tt>benchmarks/mpi</tt>.  These include the bandwidth and latency tests 
(<tt>osu_bw</tt> and <tt>osu_latency</tt>)
developed at the Ohio State University (OSU) as well as a modified version
of the bandwidth test that uses 4 nodes (<tt>crossed_osu_bw</tt>).
Perform a straightforward configure and make using MPI aware compilers 
in this directory to build the benchmarks.

<pre>
autoconf;
CC=cc ./configure --prefix=${HOME}/install/benchmarksmpi;
make; make install
</pre>

Intranode latency (between processor cores on a single socket Frankin node) is
determined by running <tt>osu_latency</tt> on two cores of a single node. 
Running the following command five times and averaging gives an intranode
latency of 0.39us.

<pre>
aprun -n 2 -N 2 ${HOME}/install/benchmarksmpi/bin/osu_latency
</pre>

Intranode bandwidth is likewise determined using <tt>osu_bw</tt>, giving
an average bandwidth of 4885.66 MB/s.

<pre>
aprun -n 2 -N 2 ${HOME}/install/benchmarksmpi/bin/osu_bw
</pre>

Running the following internode benchmarks, it is important that the MPI tasks land
on adjacent nodes in the mesh.  Node placement such as this is easier on 
some machines than others, though it looks like support will be more common
on the next generation of HPC machines.  On Franklin it is difficult to ensure
a job runs on adjacent nodes without special intervention.  Since we require at
most four adjacent nodes which the scheduler is likely to give us, it turns out
easier to just run a job and check afterwards that it did indeed run on adjacent 
nodes as desired.  The nodemap file generated earlier is useful here, just verify
that the node coordinates vary sequentially in only one direction.  One could
argue that running these benchmarks on an empty system would be preferred but
this would again require special intervention and may not be representative of
the state of the machine when traces are collected anyways.

Internode latency is determined using the same <tt>osu_latency</tt> test, 
but run with one core per node, resulting in an average latency of
5.83us.

<pre>
aprun -n 2 -N 1 ${HOME}/install/benchmarksmpi/bin/osu_latency
</pre>

The XT4 architecture presents an interesting challenge in that each node
is connected to a SeaStar router chip by a 8.0 GB/s HyperTransport bus
while the SeaStar routers themselves are connected by a 9.6 GB/s link.
Thus, we expect it to require more than one pair of nodes
performing a bandwidth test over a particular link to saturate
a router-router link.  The <tt>crossed_osu_bw</tt> benchmark has been created 
so that two pairs of nodes can perform a bandwidth test simultaneously,
configured to either use the same router-router link or not.

Performing the standard OSU bandwidth test on adjacent nodes should
be limited by the HT link, yielding an average bandwidth for the node-router
link of 1721.72 MB/s.

<pre>
aprun -n 2 -N 1 ${HOME}/install/benchmarksmpi/bin/osu_bw
</pre>

Using the <tt>crossed_osu_bw</tt> benchmark with default parameters will 
perform the bandwidth test with two pairs of nodes sharing a router-router link,
yielding a bandwidth for the router-router link of 1609.51 * 2 = 3219.02 MB/s.
Note that bandwidth is reported for one pair only, so the value must be 
multiplied by two.  This assumes that the MPI tasks where located sequentially
on adjacent nodes (verify using the nodemap file).

<pre>
aprun -n 2 -N 1 ${HOME}/install/benchmarksmpi/bin/crossed_osu_bw
</pre>

\section complicated_input More Complicated Topologies
We briefly discuss here more complicated topologies such as that on the machine Hopper at NERSC.  

Franklin is an example of a (semi)-direct network which has only a single node per switch.  In contrast,
Hopper is built on a Gemini interconnect that has two nodes per switch.  The xt2nodemap.pl script 
automatically handles this case, printing the switch coordinate in the nodemap file. If there is more than
one node per switch, the script prints the number of nodes per switch along with the mesh size.  
The sstmacro input must now look like (with parameters from Hopper inserted)
<ul>
<li> network_name = shared
<li> network_latency = 1890ns
<li> network_bandwidth_node = 4510mbytes/sec
<li> network_bandwidth_link = 7614mbytes/sec
<li> topology_geometry = 17,8,24,2
<li> topology_name = hdtorus3d
<li> nic_name = simple
</ul>
The input must specifically identify the network as an hdtorus3d and give a fourth geometry parameter 
identifying two nodes per switch.

*/
