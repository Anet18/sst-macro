<!-- !TEX root = manual.tex-->

/** \page page_Introduction Introduction

\section sec_intro_overview Overview

The SST/macro software package provides a simulator for large-scale parallel computer architectures. 
It permits the coarse-grained study of distributed-memory applications. 
The simulator is driven from either a trace file or skeleton application. 
The simulator architecture is modular, allowing it to easily be extended with additional network models, 
trace file formats, software services, and processor models.

Simulation can be broadly categorized as either off-line or on-line.
Off-line simulators typically first run a full parallel application on a real machine,
recording certain communication and computation events to a simulation trace.
This event trace can then be replayed post-mortem in the simulator.
Most common are MPI traces which record all MPI events, and
SST/macro provides the DUMPI utility (\ref sec_tutorial_dumpi) for collecting and replaying MPI traces. 
Trace extrapolation can extend the usefulness of off-line simulation by estimating large or untraceable system scales without   
having to collect a trace, it is typically only limited to strictly weak scaling.  

We turn to on-line simulation when the hardware or applications parameters need to change.
On-line simulators instead run real application code, allowing native C/C++/Fortran to be compiled directly into the simulator.
SST/macro intercepts certain function calls, estimating how much time passes rather than actually executing the function.
In MPI programs, for example, calls to MPI\_Send are linked to the simulator instead of passing to the real MPI library.
If desired, SST/macro can actually be a full MPI <i>emulator</i>, delivering messages between ranks and replicating the behavior of a full MPI implementation.

Although SST/macro supports both on-line and off-line modes, on-line simulation is encouraged because
event traces are much less flexible, containing a fixed sequence of events.
Application inputs and number of nodes cannot be changed. 
Without a flexible control flow, it also cannot simulate dynamic behavior like load balancing or faults.
On-line simulation can explore a much broader problem space since they evolve directly in the simulator.

For large, system-level experiments with thousands of network endpoints, high-accuracy cycle-accurate simulation is not possible,
or at least not convenient.
Simulation requires coarse-grained approximations to be practical.
SST/macro is therefore designed for specific cost/accuracy tradeoffs.
It should still capture complex cause/effect behavior in applications and hardware, but be efficient enough to simulate at the system-level. 
For speeding up simulator execution, we encourage <i>skeletonization</i>, discussed further in in the PDF manual. 
A high-quality skeleton is an application model that reproduces certain characteristics with only limited computation.  
We also encourage uncertainty quantification (UQ) for validating simulator results, discussed further in in the PDF manual.  
Skeletonization and UQ are the two main elements in the "canonical'' SST/macro workflow (Figure 1).

<br>

  
    \image html figures/workflow.png
      <b>Figure 1:</b> SST/macro workflow. 

<br><br>

\section sec_intro_supported Currently Supported

\subsection subsec_intro_apis Programming APIs

The following sections describe the state of the software API's (found in <i>sstmac</i>) that are available in SST/macro for use by applications, as of this release. The level of testing indicates 
the integration of compliance/functionality tests into our make check test suite.  

\subsubsection subsubsec_intro_tested Final and Tested

<ul>

<li>{MPI:} Because of its popularity, MPI is one of our main priorities in providing programming model support.  
We currently test against the MPICH test suite. All tests compile, so you should never see compilation errors.  
However, since many of the functions are not typically used in the community, we only test commonly-used functions.   
See Section \ref subsec_issues_mpi for functions that are not supported.  
Functions that are not implemented will throw a <i>sstmac::unimplemented\_error</i>, reporting the function name. 
<li>{OpenSHMEM:} Most of the standard OpenSHMEM tests pass. The ones that don't are because they haven't been ported to C++, or test the single unsupported feature (collect). 

</ul>

\subsubsection subsubsec_intro_sometesting Some testing complete

<ul>

<li>{HPX:}  HPX is an implementation of the Parallex execution model.  Some applications have been ported to it, and it has a simple test in the make check suite.  Further development and test
integration of HPX is not likely.
<li>{Sockets:} The Socket API is mostly implemented. Most basic client/server functionality is available.  However, only the default socket options are allowed. In most cases, <tt>setsockopt</tt> is just a no-op.

</ul>

\subsubsection subsubsec_intro_inutero In development

<ul>

<li>{Pthreads:}  Only the <tt>pthread\_create()</tt>, <tt>pthread\_join()</tt>, and <tt>pthread\_self()</tt> functions are implemented.  A basic  pthread test validates the core spawn/run/join behavior.
<li>{UPC:} We almost have the full UPC build and runtime implemented, but no tests are currently integrated and there are many bugs to work out before it can be used. 
<li>{GNI:}  Cray's low-level messaging interface, GNI, is being implemented.

</ul>

\subsection subsec_intro_toolsandstats Analysis Tools and Statistics
The following analysis tools are currently available in SST/macro.
Some are thoroughly tested. Others have undergone some testing, but are still considered Beta.  Others have been implemented, but are relatively untested.
\subsubsection subsubsec_intro_fulltestedtools Fully tested
<ul>

<li> Call graph: Generates callgrind.out file that can be visualized in either KCacheGrind or QCacheGrind. More details are given in \ref sec_tutorials_callgraph.
<li> Spyplot: Generates .csv data files tabulating the number of messages and number of bytes sent between MPI ranks. SST/macro can also directly generate a PNG file. Otherwise, the .csv files can be visualized in the plotting program Scilab. More details are given in \ref sec_tutorials_spyplot.
<li> Fixed-time quanta (FTQ): Generates a .csv data tabulating the amount of time spent doing computation/communication as the application progresses along with a Gnuplot script for visualization as a histogram. More details are given in \ref sec_tutorials_ftq

</ul>

\subsubsection subsubsec_intro_betatools Beta
<ul>

<li> Trace analysis: With the traceanalyzer executable, fine-grained metrics for characterizing application execution can be output.

</ul>

\subsubsection subsubsec_intro_untestedtools Untested
<ul>

<li> Congestion: With the <tt>-d "<stats> congestion"</tt> command line option, SST/macro will dump statistics for network congestion on individual links (packet train model only).

</ul>

\section sec_intro_issues Known Issues and Limitations

\subsection subsec_issues_globals Global Variables

The use of global variables in SST/macro inherently creates a false-sharing scenario 
because of the use of user-space threads to model parallel processes.   
While we do have a mechanism for supporting them (in the PDF manual for more information), 
the file using them must be compiled with C++.   
This is somewhat unfortunate, because many C programs will use global variables as a convenient means of accessing program data.   
In almost every case, though, a C program can simply be compiled as C++ by changing the extension to .cc or .cpp.

\subsection subsec_issues_mpi MPI

Everything from MPI 2 is implemented with a few exceptions noted below.  
The following are <i>not</i> implemented (categorized by MPI concepts):

\subsubsection subsubsec_issues_mpi_comm Communicators

<ul>

<li> Anything using or having to do with Inter-communicators (<tt>MPI\_Intercomm\_create()</tt>)
<li> Topology communicators

</ul>

\subsubsection subsubsec_issues_mpi_types Datatypes and Addressing

<ul>

<li> Complicated use of MPI\_LB and MPI\_UB to define a struct, and collections of structs (MPI test 138). 
<li> Changing the name of built-in datatypes with <tt>MPI\_Type\_set\_name()</tt> (MPI test 171).
<li> <tt>MPI\_Create\_darray()</tt>, <tt>MPI\_Create\_subarray()</tt>, and <tt>MPI\_Create\_resized()</tt>
<li> <tt>MPI\_Pack\_external() </tt>, which is only useful for sending messages across MPI implementations apparently.
<li> <tt>MPI\_Type\_match\_size() </tt>  - extended fortran support
<li> Use of MPI\_BOTTOM (relative addressing).  Use normal buffers. 
<li> Using Fortran types (e.g. MPI\_COMLEX) from C.

</ul>


\subsubsection subsubsec_issues_mpi_info Info and Attributes

No <tt>MPI\_Info\_*</tt>, <tt>MPI\_*\_keyval</tt>, or <tt>MPI\_Attr\_*</tt> functions are supported.

\subsubsection subsubsec_issues_mpi_ptpt Point-to-Point

<ul>

<li> <tt>MPI\_Grequest\_*</tt> functions (generalized requests).
<li> Use of testing non-blocking functions in a loop, such as:


\code{.cpp}

while(!flag)
{
  MPI_Iprobe( 0, 0, MPI_COMM_WORLD, &flag, &status );
}

\endcode

For some configurations, simulation time never advances in the MPI\_Iprobe call. 
This causes an infinite loop that never returns to the discrete event manager. 
Even if configured so that time progresses, the code will work but will take a very long time to run.
	

</ul>


\subsubsection subsubsec_issues_mpi_collectives Collectives

<ul>

<li> There seems to be a problem with using MPI\_FLOAT and MPI\_PROD in <tt>MPI\_Allreduce()</tt> (MPI test 22)
<li> There seems to be a problem with using non-commutative user-defined operators in <tt>MPI\_Reduce()</tt> and <tt>MPI\_Allreduce()</tt>.
<li> <tt>MPI\_Alltoallw()</tt> is not implemented
<li> <tt>MPI\_Exscan()</tt> is not implemented
<li> <tt>MPI\_Reduce\_Scatter\_block()</tt> is not implemented.
<li> <tt>MPIX\_*</tt> functions are not implemented  (like non-blocking collectives).
<li> Calling MPI functions from user-defined reduce operations (MPI test 39; including <tt>MPI\_Comm\_rank</tt>).

</ul>

\subsubsection subsubsec_issues_mpi_misc Miscellaneous

<ul>

<li> <tt>MPI\_Is\_thread\_main()</tt> is not implemented.

</ul>

\subsection subsec_issues_shmem OpenSHMEM

Only the collect and fcollect functions of the API are not implemented (they will be in future releases).

Also, like handling of global variables discussed in Sections \ref subsec_issues_globals in the PDF manual, SHMEM globals need to use a different type.  
For primitives and primitive arrays, refer to <tt><sstmac/openshmem/shmem/globals.h></tt> for replacing types, e.g. the following code

\code{.sh}

int my_global_var = 4;
double an_array[6];

\endcode

needs to become

\code{.sh}

shmem_int my_global_var(4);
shmem_arr<double, 6> an_array;

\endcode


\subsection subsec_issues_fortran Fortran

SST/macro can run Fortran90 applications.  
However, at least using gfortran, Fortran variables using allocate() go on the heap.  
Therefore, it creates a false sharing situation pretty much everywhere as threads swap in and out.  
A workaround is to make a big map full of data structures that store needed variables, 
indexed by a rank that you pass around to every function.  
We are exploring more user-friendly alternatives.  The Fortran MPI interface is also still somewhat incomplete.
Most functions are just wrappers to the C/C++ implementation and we are working on adding the bindings.


*/